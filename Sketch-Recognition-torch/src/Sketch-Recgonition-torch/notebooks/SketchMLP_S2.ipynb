{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bi1q6cQCVaUn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "654b5287-b943-48ff-a546-9e604af3a1bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Processing NPY files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 30/30 [00:20<00:00,  1.50it/s]\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training dataset size: 480000\n",
            "Validation dataset size: 120000\n",
            "SketchMLP_S2(\n",
            "  (patch_embed): Conv2d(1, 96, kernel_size=(4, 4), stride=(4, 4))\n",
            "  (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
            "  (stages): ModuleList(\n",
            "    (0): Sequential(\n",
            "      (0): SpatialShiftMLP(\n",
            "        (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
            "        (act): GELU(approximate='none')\n",
            "        (drop1): Dropout(p=0.1, inplace=False)\n",
            "        (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
            "        (drop2): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): GELU(approximate='none')\n",
            "      (3): SpatialShiftMLP(\n",
            "        (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
            "        (act): GELU(approximate='none')\n",
            "        (drop1): Dropout(p=0.1, inplace=False)\n",
            "        (fc2): Linear(in_features=384, out_features=192, bias=True)\n",
            "        (drop2): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (5): GELU(approximate='none')\n",
            "    )\n",
            "    (1): Sequential(\n",
            "      (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "      (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): GELU(approximate='none')\n",
            "    )\n",
            "    (2): Sequential(\n",
            "      (0): SpatialShiftMLP(\n",
            "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
            "        (act): GELU(approximate='none')\n",
            "        (drop1): Dropout(p=0.1, inplace=False)\n",
            "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
            "        (drop2): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): GELU(approximate='none')\n",
            "      (3): SpatialShiftMLP(\n",
            "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
            "        (act): GELU(approximate='none')\n",
            "        (drop1): Dropout(p=0.1, inplace=False)\n",
            "        (fc2): Linear(in_features=768, out_features=384, bias=True)\n",
            "        (drop2): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (5): GELU(approximate='none')\n",
            "    )\n",
            "    (3): Sequential(\n",
            "      (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "      (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): GELU(approximate='none')\n",
            "    )\n",
            "    (4): Sequential(\n",
            "      (0): SpatialShiftMLP(\n",
            "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
            "        (act): GELU(approximate='none')\n",
            "        (drop1): Dropout(p=0.1, inplace=False)\n",
            "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "        (drop2): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): GELU(approximate='none')\n",
            "      (3): SpatialShiftMLP(\n",
            "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
            "        (act): GELU(approximate='none')\n",
            "        (drop1): Dropout(p=0.1, inplace=False)\n",
            "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "        (drop2): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (5): GELU(approximate='none')\n",
            "      (6): SpatialShiftMLP(\n",
            "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
            "        (act): GELU(approximate='none')\n",
            "        (drop1): Dropout(p=0.1, inplace=False)\n",
            "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "        (drop2): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (7): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (8): GELU(approximate='none')\n",
            "      (9): SpatialShiftMLP(\n",
            "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
            "        (act): GELU(approximate='none')\n",
            "        (drop1): Dropout(p=0.1, inplace=False)\n",
            "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "        (drop2): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (10): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (11): GELU(approximate='none')\n",
            "      (12): SpatialShiftMLP(\n",
            "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
            "        (act): GELU(approximate='none')\n",
            "        (drop1): Dropout(p=0.1, inplace=False)\n",
            "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "        (drop2): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (13): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (14): GELU(approximate='none')\n",
            "      (15): SpatialShiftMLP(\n",
            "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
            "        (act): GELU(approximate='none')\n",
            "        (drop1): Dropout(p=0.1, inplace=False)\n",
            "        (fc2): Linear(in_features=1536, out_features=768, bias=True)\n",
            "        (drop2): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (16): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (17): GELU(approximate='none')\n",
            "    )\n",
            "    (5): Sequential(\n",
            "      (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "      (1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): GELU(approximate='none')\n",
            "    )\n",
            "    (6): Sequential(\n",
            "      (0): SpatialShiftMLP(\n",
            "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        (act): GELU(approximate='none')\n",
            "        (drop1): Dropout(p=0.1, inplace=False)\n",
            "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "        (drop2): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): GELU(approximate='none')\n",
            "      (3): SpatialShiftMLP(\n",
            "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        (act): GELU(approximate='none')\n",
            "        (drop1): Dropout(p=0.1, inplace=False)\n",
            "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "        (drop2): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (4): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (5): GELU(approximate='none')\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (head): Linear(in_features=768, out_features=30, bias=True)\n",
            ")\n",
            "Total parameters: 25055646\n",
            "Training complete!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "import requests\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "# Constants\n",
        "CLASSES = [\n",
        "    \"apple\", \"banana\", \"book\", \"car\", \"cat\", \"chair\", \"cloud\", \"dog\", \"door\", \"eye\",\n",
        "    \"face\", \"fish\", \"flower\", \"fork\", \"guitar\", \"hammer\", \"hat\", \"house\", \"key\", \"knife\",\n",
        "    \"leaf\", \"lightning\", \"moon\", \"mountain\", \"mouse\", \"star\", \"sun\", \"table\", \"tree\", \"umbrella\"\n",
        "]\n",
        "NUM_CLASSES = len(CLASSES)\n",
        "BATCH_SIZE = 128\n",
        "IMAGE_SIZE = 224\n",
        "NUM_EPOCHS = 100\n",
        "LEARNING_RATE = 0.0001\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Create directories for storing datasets\n",
        "def create_directories():\n",
        "    os.makedirs(\"./drive/MyDrive/data_npy/\", exist_ok=True)\n",
        "    os.makedirs(\"./drive/MyDrive/data_npy/npy/\", exist_ok=True)\n",
        "    os.makedirs(\"./drive/MyDrive/data_npy/processed/\", exist_ok=True)\n",
        "    for class_name in CLASSES:\n",
        "        os.makedirs(f\"data/processed/{class_name}\", exist_ok=True)\n",
        "\n",
        "# Download QuickDraw NPY dataset\n",
        "def download_quickdraw_npy_dataset():\n",
        "    print(\"Downloading QuickDraw NPY dataset for sketch recognition...\")\n",
        "    for class_name in tqdm(CLASSES):\n",
        "        try:\n",
        "            # QuickDraw dataset NPY URL format\n",
        "            url = f\"https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/{class_name}.npy\"\n",
        "            response = requests.get(url, stream=True)\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                # Save NPY file\n",
        "                with open(f\"./drive/MyDrive/data_npy/npy/{class_name}.npy\", 'wb') as f:\n",
        "                    f.write(response.content)\n",
        "\n",
        "                print(f\"Downloaded NPY file for class {class_name}\")\n",
        "            else:\n",
        "                print(f\"Failed to download {class_name} NPY. Status code: {response.status_code}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error downloading {class_name}: {e}\")\n",
        "\n",
        "# Process NPY files to create normalized tensors or images if needed\n",
        "def process_npy_files(max_samples_per_class=20000):\n",
        "    print(\"Processing NPY files...\")\n",
        "    class_data = {}\n",
        "\n",
        "    for class_name in tqdm(CLASSES):\n",
        "        npy_path = f\"./drive/MyDrive/data_npy/npy/{class_name}.npy\"\n",
        "        if os.path.exists(npy_path):\n",
        "            # Load NPY file - contains bitmap data in shape [N, 784]\n",
        "            sketches = np.load(npy_path)\n",
        "\n",
        "            # Limit samples per class\n",
        "            if len(sketches) > max_samples_per_class:\n",
        "                # Random sampling to get variety\n",
        "                indices = np.random.choice(len(sketches), max_samples_per_class, replace=False)\n",
        "                sketches = sketches[indices]\n",
        "\n",
        "            # Reshape from 784 to 28x28\n",
        "            sketches = sketches.reshape(-1, 28, 28)\n",
        "\n",
        "            # Store processed data\n",
        "            class_data[class_name] = sketches\n",
        "\n",
        "            # Optionally save as images for visualization\n",
        "            # for i, sketch in enumerate(sketches[:10]):  # Save first 10 for visualization\n",
        "            #     img = Image.fromarray(sketch).resize((IMAGE_SIZE, IMAGE_SIZE), Image.BILINEAR)\n",
        "            #     img.save(f\"data/processed/{class_name}/{i}.png\")\n",
        "\n",
        "    return class_data\n",
        "\n",
        "# Custom Dataset class for NPY sketches\n",
        "class QuickDrawNPYDataset(Dataset):\n",
        "    def __init__(self, class_data, class_to_idx, transform=None):\n",
        "        self.transform = transform\n",
        "        self.samples = []\n",
        "        self.targets = []\n",
        "\n",
        "        # Build dataset from class_data dictionary\n",
        "        for class_name, sketches in class_data.items():\n",
        "            class_idx = class_to_idx[class_name]\n",
        "            for sketch in sketches:\n",
        "                self.samples.append(sketch)\n",
        "                self.targets.append(class_idx)\n",
        "\n",
        "        self.samples = np.array(self.samples)\n",
        "        self.targets = np.array(self.targets)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sketch = self.samples[idx]\n",
        "        label = self.targets[idx]\n",
        "\n",
        "        # Resize from 28x28 to target size\n",
        "        sketch_resized = np.array(Image.fromarray(sketch).resize((IMAGE_SIZE, IMAGE_SIZE), Image.BILINEAR))\n",
        "\n",
        "        # Convert to tensor and keep as single channel\n",
        "        sketch_tensor = torch.from_numpy(sketch_resized).float() / 255.0\n",
        "        sketch_tensor = sketch_tensor.unsqueeze(0)  # Add channel dimension [1, H, W]\n",
        "\n",
        "        if self.transform:\n",
        "            sketch_tensor = self.transform(sketch_tensor)\n",
        "\n",
        "        return sketch_tensor, label\n",
        "\n",
        "# Data transformations\n",
        "def get_transforms():\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomRotation(10),\n",
        "        transforms.Normalize(mean=[0.5], std=[0.5])  # For grayscale\n",
        "    ])\n",
        "\n",
        "    test_transform = transforms.Compose([\n",
        "        transforms.Normalize(mean=[0.5], std=[0.5])  # For grayscale\n",
        "    ])\n",
        "\n",
        "    return train_transform, test_transform\n",
        "\n",
        "# Spatial Shift MLP Module\n",
        "class SpatialShiftMLP(nn.Module):\n",
        "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
        "        super().__init__()\n",
        "        out_features = out_features or in_features\n",
        "        hidden_features = hidden_features or in_features\n",
        "\n",
        "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
        "        self.act = act_layer()\n",
        "        self.drop1 = nn.Dropout(drop)\n",
        "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
        "        self.drop2 = nn.Dropout(drop)\n",
        "        self.out_features = out_features  # Track output channels\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.shape\n",
        "\n",
        "        # [Rest of spatial shift operations remain the same]\n",
        "        # Spatial-shift operation\n",
        "        x_shifted = x.clone()\n",
        "\n",
        "        # For grayscale, we can use different patterns for spatial shifting\n",
        "        if C == 1:\n",
        "            # Split the spatial dimensions into quadrants and shift differently\n",
        "            h_mid, w_mid = H // 2, W // 2\n",
        "\n",
        "            # Shift top-left quadrant right\n",
        "            x_shifted[:, :, :h_mid, :-1] = x[:, :, :h_mid, 1:]\n",
        "\n",
        "            # Shift top-right quadrant left\n",
        "            x_shifted[:, :, :h_mid, 1:] = x[:, :, :h_mid, :-1]\n",
        "\n",
        "            # Shift bottom-left quadrant down\n",
        "            x_shifted[:, :, :-1, :w_mid] = x[:, :, 1:, :w_mid]\n",
        "\n",
        "            # Shift bottom-right quadrant up\n",
        "            x_shifted[:, :, 1:, w_mid:] = x[:, :, :-1, w_mid:]\n",
        "        else:\n",
        "            # Original shift pattern for multiple channels\n",
        "            chunk_size = max(1, C // 4)\n",
        "\n",
        "            # Split the channels and shift in different directions\n",
        "            # For the cases where C < 4, we adjust accordingly\n",
        "            if C >= 1:\n",
        "                # Group 1: shift left\n",
        "                end_idx = min(chunk_size, C)\n",
        "                x_shifted[:, :end_idx, :, 1:] = x[:, :end_idx, :, :-1]\n",
        "\n",
        "            if C >= 2:\n",
        "                # Group 2: shift right\n",
        "                end_idx = min(chunk_size*2, C)\n",
        "                x_shifted[:, chunk_size:end_idx, :, :-1] = x[:, chunk_size:end_idx, :, 1:]\n",
        "\n",
        "            if C >= 3:\n",
        "                # Group 3: shift up\n",
        "                end_idx = min(chunk_size*3, C)\n",
        "                x_shifted[:, chunk_size*2:end_idx, 1:, :] = x[:, chunk_size*2:end_idx, :-1, :]\n",
        "\n",
        "            if C >= 4:\n",
        "                # Group 4: shift down\n",
        "                x_shifted[:, chunk_size*3:, :-1, :] = x[:, chunk_size*3:, 1:, :]\n",
        "\n",
        "\n",
        "        # Reshape for MLP: [B, C, H, W] -> [B, H*W, C]\n",
        "        x_reshaped = x_shifted.permute(0, 2, 3, 1).reshape(B, H*W, C)\n",
        "\n",
        "        # MLP layers\n",
        "        x_reshaped = self.fc1(x_reshaped)\n",
        "        x_reshaped = self.act(x_reshaped)\n",
        "        x_reshaped = self.drop1(x_reshaped)\n",
        "        x_reshaped = self.fc2(x_reshaped)\n",
        "        x_reshaped = self.drop2(x_reshaped)\n",
        "\n",
        "        # Reshape back using updated channel count\n",
        "        x_out = x_reshaped.reshape(B, H, W, self.out_features).permute(0, 3, 1, 2)\n",
        "\n",
        "        return x_out\n",
        "\n",
        "# Modified SketchMLP_S2 class\n",
        "class SketchMLP_S2(nn.Module):\n",
        "    def __init__(self, img_size=224, patch_size=4, in_chans=1, num_classes=30,\n",
        "                 embed_dim=96, depths=[2, 2, 6, 2], mlp_ratio=4., drop_rate=0.):\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "        # Patch embedding\n",
        "        self.patch_embed = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "\n",
        "        # Build MLP stages with proper channel progression\n",
        "        self.stages = nn.ModuleList()\n",
        "        current_dim = embed_dim\n",
        "        for i, depth in enumerate(depths):\n",
        "            stage = nn.Sequential()\n",
        "            for j in range(depth):\n",
        "                # Calculate output dimension\n",
        "                out_dim = current_dim * 2 if j == depth-1 and i < len(depths)-1 else current_dim\n",
        "\n",
        "                stage.append(SpatialShiftMLP(\n",
        "                    in_features=current_dim,\n",
        "                    hidden_features=int(current_dim * mlp_ratio),\n",
        "                    out_features=out_dim,\n",
        "                    drop=drop_rate\n",
        "                ))\n",
        "\n",
        "                # Update current dimension\n",
        "                if j == depth-1 and i < len(depths)-1:\n",
        "                    current_dim = out_dim\n",
        "\n",
        "                # Add normalization\n",
        "                stage.append(nn.BatchNorm2d(out_dim))\n",
        "                stage.append(nn.GELU())\n",
        "\n",
        "            self.stages.append(stage)\n",
        "\n",
        "            # Add downsampling\n",
        "            if i < len(depths) - 1:\n",
        "                self.stages.append(nn.Sequential(\n",
        "                    nn.Conv2d(current_dim, current_dim, kernel_size=3, stride=2, padding=1),\n",
        "                    nn.BatchNorm2d(current_dim),\n",
        "                    nn.GELU()\n",
        "                ))\n",
        "\n",
        "        # Final classification head\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.flatten = nn.Flatten(1)\n",
        "        self.head = nn.Linear(current_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Patch embedding\n",
        "        x = self.patch_embed(x)\n",
        "\n",
        "        # Process through MLP stages\n",
        "        for stage in self.stages:\n",
        "            x = stage(x)\n",
        "\n",
        "        # Classification head\n",
        "        x = self.avgpool(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.head(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "# Training function\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs):\n",
        "    model.to(DEVICE)\n",
        "    best_val_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}')\n",
        "        for inputs, labels in progress_bar:\n",
        "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
        "\n",
        "            # Zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Backward + optimize\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Statistics\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "            progress_bar.set_postfix({\n",
        "                'loss': running_loss/total,\n",
        "                'acc': 100.*correct/total\n",
        "            })\n",
        "\n",
        "        # Step scheduler\n",
        "        scheduler.step()\n",
        "\n",
        "        # Validation\n",
        "        val_loss, val_acc = evaluate_model(model, val_loader, criterion)\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {running_loss/total:.4f}, Train Acc: {100.*correct/total:.2f}%, '\n",
        "              f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
        "\n",
        "        # Save best model\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            torch.save(model.state_dict(), '/content/drive/MyDrive/sketchmlp_models/best_sketchMLP_model.pth')\n",
        "            print(f'Model saved with validation accuracy: {val_acc:.2f}%')\n",
        "\n",
        "    return model\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate_model(model, dataloader, criterion):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    return running_loss/total, 100.*correct/total\n",
        "\n",
        "# Visualize some sample sketches from the dataset\n",
        "def visualize_samples(class_data, num_samples=5, classes_to_show=5):\n",
        "    plt.figure(figsize=(15, 10))\n",
        "\n",
        "    for i, class_name in enumerate(list(class_data.keys())[:classes_to_show]):\n",
        "        sketches = class_data[class_name][:num_samples]\n",
        "\n",
        "        for j, sketch in enumerate(sketches):\n",
        "            plt.subplot(classes_to_show, num_samples, i*num_samples + j + 1)\n",
        "            plt.imshow(sketch, cmap='gray')\n",
        "            plt.title(class_name if j == 0 else \"\")\n",
        "            plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('/content/drive/MyDrive/sketchmlp_models/sample_sketches.png')\n",
        "    plt.close()\n",
        "\n",
        "# Visualize predictions\n",
        "def visualize_predictions(model, dataloader, num_images=10):\n",
        "    model.eval()\n",
        "    images_so_far = 0\n",
        "    fig = plt.figure(figsize=(15, 8))\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            for j in range(inputs.size()[0]):\n",
        "                images_so_far += 1\n",
        "                ax = plt.subplot(2, num_images//2, images_so_far)\n",
        "                ax.axis('off')\n",
        "                ax.set_title(f'Pred: {CLASSES[preds[j]]}\\nTrue: {CLASSES[labels[j]]}')\n",
        "\n",
        "                # Display the grayscale image\n",
        "                img = inputs.cpu().data[j, 0].numpy()\n",
        "                # Denormalize\n",
        "                img = img * 0.5 + 0.5\n",
        "                img = np.clip(img, 0, 1)\n",
        "\n",
        "                ax.imshow(img, cmap='gray')\n",
        "\n",
        "                if images_so_far == num_images:\n",
        "                    plt.tight_layout()\n",
        "                    plt.savefig('/content/drive/MyDrive/sketchmlp_models/prediction_results.png')\n",
        "                    return\n",
        "\n",
        "# Main execution\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "# Create directories\n",
        "create_directories()\n",
        "\n",
        "# Download QuickDraw NPY dataset\n",
        "# download_quickdraw_npy_dataset()\n",
        "\n",
        "# Process NPY files\n",
        "class_data = process_npy_files(max_samples_per_class=20000)\n",
        "\n",
        "# Visualize some samples\n",
        "visualize_samples(class_data)\n",
        "\n",
        "# Create class to index mapping\n",
        "class_to_idx = {cls_name: idx for idx, cls_name in enumerate(CLASSES)}\n",
        "\n",
        "# Data transforms\n",
        "train_transform, test_transform = get_transforms()\n",
        "\n",
        "# Create full dataset\n",
        "full_dataset = QuickDrawNPYDataset(class_data, class_to_idx, transform=None)\n",
        "\n",
        "# Split dataset into train and validation\n",
        "train_size = int(0.8 * len(full_dataset))\n",
        "val_size = len(full_dataset) - train_size\n",
        "\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(\n",
        "    full_dataset, [train_size, val_size],\n",
        "    generator=torch.Generator().manual_seed(42)  # For reproducibility\n",
        ")\n",
        "\n",
        "# Create custom datasets with proper transforms\n",
        "class TransformedSubset(Dataset):\n",
        "    def __init__(self, subset, transform=None):\n",
        "        self.subset = subset\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x, y = self.subset[idx]\n",
        "        if self.transform:\n",
        "            x = self.transform(x)\n",
        "        return x, y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.subset)\n",
        "\n",
        "train_dataset = TransformedSubset(train_dataset, train_transform)\n",
        "val_dataset = TransformedSubset(val_dataset, test_transform)\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
        "\n",
        "print(f\"Training dataset size: {len(train_dataset)}\")\n",
        "print(f\"Validation dataset size: {len(val_dataset)}\")\n",
        "\n",
        "# Initialize model\n",
        "model = SketchMLP_S2(\n",
        "    img_size=IMAGE_SIZE,\n",
        "    patch_size=4,\n",
        "    in_chans=1,  # Single channel for grayscale\n",
        "    num_classes=NUM_CLASSES,\n",
        "    embed_dim=96,\n",
        "    depths=[2, 2, 6, 2],\n",
        "    mlp_ratio=4.,\n",
        "    drop_rate=0.1\n",
        ")\n",
        "\n",
        "# Print model summary\n",
        "print(model)\n",
        "print(f\"Total parameters: {sum(p.numel() for p in model.parameters())}\")\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.05)\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS)\n",
        "\n",
        "# Train model\n",
        "# trained_model = train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, NUM_EPOCHS)\n",
        "\n",
        "# Evaluate on validation set\n",
        "# val_loss, val_acc = evaluate_model(trained_model, val_loader, criterion)\n",
        "# print(f'Final Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.2f}%')\n",
        "\n",
        "# Visualize some predictions\n",
        "# visualize_predictions(trained_model, val_loader)\n",
        "\n",
        "print(\"Training complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to resume training from a saved checkpoint\n",
        "def resume_training(saved_model_path, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, start_epoch=0):\n",
        "    # Load the saved model\n",
        "    model = SketchMLP_S2(\n",
        "        img_size=IMAGE_SIZE,\n",
        "        patch_size=4,\n",
        "        in_chans=1,\n",
        "        num_classes=NUM_CLASSES,\n",
        "        embed_dim=96,\n",
        "        depths=[2, 2, 6, 2],\n",
        "        mlp_ratio=4.,\n",
        "        drop_rate=0.1\n",
        "    )\n",
        "    model.load_state_dict(torch.load(saved_model_path))\n",
        "    model.to(DEVICE)\n",
        "\n",
        "    print(f\"Resumed training from epoch {start_epoch + 1}\")\n",
        "\n",
        "    # Resume training\n",
        "    best_val_acc = 0.0\n",
        "    for epoch in range(start_epoch, num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}')\n",
        "        for inputs, labels in progress_bar:\n",
        "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "            progress_bar.set_postfix({\n",
        "                'loss': running_loss / total,\n",
        "                'acc': 100. * correct / total\n",
        "            })\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        # Validation\n",
        "        val_loss, val_acc = evaluate_model(model, val_loader, criterion)\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {running_loss/total:.4f}, Train Acc: {100.*correct/total:.2f}%, '\n",
        "              f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
        "\n",
        "        # Save best model\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            torch.save(model.state_dict(), '/content/drive/MyDrive/sketchmlp_models/best_sketchMLP_model.pth')\n",
        "            print(f'Model saved with validation accuracy: {val_acc:.2f}%')\n",
        "\n",
        "    return model\n",
        "\n",
        "# Example of resuming training\n",
        "if __name__ == \"__main__\":\n",
        "    # Define the saved model path and optimizer path (if applicable)\n",
        "    saved_model_path = '/content/drive/MyDrive/sketchmlp_models/best_sketchMLP_model.pth'\n",
        "\n",
        "    # Create data loaders as before\n",
        "    # train_loader and val_loader need to be initialized as before\n",
        "\n",
        "    # Define criterion, optimizer, and scheduler\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.05)\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS)\n",
        "\n",
        "    # Resume training from epoch 10\n",
        "    trained_model = resume_training(saved_model_path, train_loader, val_loader, criterion, optimizer, scheduler, NUM_EPOCHS, start_epoch=10)\n"
      ],
      "metadata": {
        "id": "fmL3gX2Y3JkK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6bf79cdc-e7f2-41f3-fad0-2ced2e700ef2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resumed training from epoch 11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11/100: 100%|██████████| 3750/3750 [1:27:32<00:00,  1.40s/it, loss=0.264, acc=92.5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11/100, Train Loss: 0.2643, Train Acc: 92.50%, Val Loss: 0.2438, Val Acc: 93.17%\n",
            "Model saved with validation accuracy: 93.17%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12/100:  17%|█▋        | 640/3750 [14:57<1:12:36,  1.40s/it, loss=0.266, acc=92.5]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PG-XTodlQfLV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}