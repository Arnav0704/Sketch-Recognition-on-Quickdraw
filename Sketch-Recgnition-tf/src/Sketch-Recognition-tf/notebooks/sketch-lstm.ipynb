{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Hybrid Approach with CNN and LSTM \n",
    "- top 1 accuracy: ~94%\n",
    "- top 3 accuracy: ~98%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "import numpy as np\n",
    "\n",
    "# Create a directory to store the data\n",
    "# if not os.path.exists('quickdraw_data'):\n",
    "#     os.makedirs('quickdraw_data')\n",
    "\n",
    "# --- Constants ---\n",
    "# Base URL for simplified drawings\n",
    "BASE_URL = \"https://storage.googleapis.com/quickdraw_dataset/full/simplified/\"\n",
    "\n",
    "# Select 30 classes (adjust as needed)\n",
    "# Example list:\n",
    "CLASSES = [\n",
    "    \"apple\", \"banana\", \"book\", \"car\", \"cat\", \"chair\", \"cloud\", \"dog\", \"door\", \"eye\",\n",
    "    \"face\", \"fish\", \"flower\", \"fork\", \"guitar\", \"hammer\", \"hat\", \"house\", \"key\", \"knife\",\n",
    "    \"leaf\", \"lightning\", \"moon\", \"mountain\", \"mouse\", \"star\", \"sun\", \"table\", \"tree\", \"umbrella\"\n",
    "]\n",
    "NUM_CLASSES = len(CLASSES)\n",
    "SAMPLES_PER_CLASS = 10000 # Limit samples per class for faster demo/training\n",
    "DATA_DIR = \"path/to/your/data/directory\" # Change this to your desired directory\n",
    "MAX_LEN = 196 # Max number of points in a sequence (adjust based on data analysis)\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 100 # Adjust for real training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-27T05:38:16.453926Z",
     "iopub.status.busy": "2025-04-27T05:38:16.453410Z",
     "iopub.status.idle": "2025-04-27T05:38:16.462285Z",
     "shell.execute_reply": "2025-04-27T05:38:16.461688Z",
     "shell.execute_reply.started": "2025-04-27T05:38:16.453897Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data should be downloaded in the '/kaggle/input/quick-draw-ndjson' directory.\n"
     ]
    }
   ],
   "source": [
    "def download_data(classes, base_url, data_dir, samples_per_class):\n",
    "    \"\"\"Downloads .ndjson files for specified classes.\"\"\"\n",
    "    print(\"Starting download...\")\n",
    "    for class_name in classes:\n",
    "        class_name_url = class_name.replace(\" \", \"%20\") # Handle spaces in names if any\n",
    "        file_path = os.path.join(data_dir, f\"{class_name}.ndjson\")\n",
    "\n",
    "        if os.path.exists(file_path):\n",
    "            print(f\"File for '{class_name}' already exists. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        url = f\"{base_url}{class_name_url}.ndjson\"\n",
    "        print(f\"Downloading {class_name} from {url}...\")\n",
    "        try:\n",
    "            response = requests.get(url, stream=True)\n",
    "            response.raise_for_status() # Raise an exception for bad status codes\n",
    "\n",
    "            with open(file_path, 'wb') as f:\n",
    "                 for chunk in response.iter_content(chunk_size=8192):\n",
    "                     f.write(chunk)\n",
    "            print(f\"Downloaded '{class_name}'\")\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error downloading {class_name}: {e}\")\n",
    "            # Remove partially downloaded file if error occurs\n",
    "            if os.path.exists(file_path):\n",
    "                os.remove(file_path)\n",
    "        except Exception as e:\n",
    "             print(f\"An unexpected error occurred for {class_name}: {e}\")\n",
    "             if os.path.exists(file_path):\n",
    "                os.remove(file_path)\n",
    "\n",
    "    print(\"Download process finished.\")\n",
    "\n",
    "# --- Download the data ---\n",
    "download_data(CLASSES, BASE_URL, DATA_DIR, SAMPLES_PER_CLASS)\n",
    "\n",
    "print(f\"\\nData should be downloaded in the '{DATA_DIR}' directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T05:38:18.256604Z",
     "iopub.status.busy": "2025-04-27T05:38:18.256355Z",
     "iopub.status.idle": "2025-04-27T05:38:39.964786Z",
     "shell.execute_reply": "2025-04-27T05:38:39.964133Z",
     "shell.execute_reply.started": "2025-04-27T05:38:18.256584Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing data...\n",
      "Loaded 10000 samples for 'apple'\n",
      "Loaded 10000 samples for 'banana'\n",
      "Loaded 10000 samples for 'book'\n",
      "Loaded 10000 samples for 'car'\n",
      "Loaded 10000 samples for 'cat'\n",
      "Loaded 10000 samples for 'chair'\n",
      "Loaded 10000 samples for 'cloud'\n",
      "Loaded 10000 samples for 'dog'\n",
      "Loaded 10000 samples for 'door'\n",
      "Loaded 10000 samples for 'eye'\n",
      "Loaded 10000 samples for 'face'\n",
      "Loaded 10000 samples for 'fish'\n",
      "Loaded 10000 samples for 'flower'\n",
      "Loaded 10000 samples for 'fork'\n",
      "Loaded 10000 samples for 'guitar'\n",
      "Loaded 10000 samples for 'hammer'\n",
      "Loaded 10000 samples for 'hat'\n",
      "Loaded 10000 samples for 'house'\n",
      "Loaded 10000 samples for 'key'\n",
      "Loaded 10000 samples for 'knife'\n",
      "Loaded 10000 samples for 'leaf'\n",
      "Loaded 10000 samples for 'lightning'\n",
      "Loaded 10000 samples for 'moon'\n",
      "Loaded 10000 samples for 'mountain'\n",
      "Loaded 10000 samples for 'mouse'\n",
      "Loaded 10000 samples for 'star'\n",
      "Loaded 10000 samples for 'sun'\n",
      "Loaded 10000 samples for 'table'\n",
      "Loaded 10000 samples for 'tree'\n",
      "Loaded 10000 samples for 'umbrella'\n",
      "Total sequences loaded: 300000\n",
      "Normalization - Mean (dx, dy): [3.0792246 2.8368876], Std Dev (dx, dy): [36.755478 37.947872]\n",
      "Data shapes - Sequences: (300000, 196, 3), Labels: (300000, 30)\n",
      "Train set size: 240000, Validation set size: 60000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow as tf\n",
    "\n",
    "# --- Load and Preprocess Functions ---\n",
    "def strokes_to_deltas(drawing_strokes):\n",
    "    \"\"\"\n",
    "    Converts raw strokes list [ [[x,...],[y,...]], ...]\n",
    "    to delta format [(dx, dy, pen_state), ...].\n",
    "    pen_state = 0 for intermediate points, 1 for last point in stroke.\n",
    "    \"\"\"\n",
    "    deltas = []\n",
    "    last_x, last_y = 0, 0\n",
    "    for stroke in drawing_strokes:\n",
    "        x_coords, y_coords = stroke[0], stroke[1]\n",
    "        if not x_coords: # Skip empty strokes if any\n",
    "            continue\n",
    "\n",
    "        # First point uses absolute coords (or diff from 0,0)\n",
    "        dx = x_coords[0] - last_x\n",
    "        dy = y_coords[0] - last_y\n",
    "        deltas.append([dx, dy, 0]) # pen_state=0 for first point\n",
    "\n",
    "        # Subsequent points use deltas\n",
    "        for i in range(1, len(x_coords)):\n",
    "            dx = x_coords[i] - x_coords[i-1]\n",
    "            dy = y_coords[i] - y_coords[i-1]\n",
    "            deltas.append([dx, dy, 0]) # pen_state=0 for intermediate\n",
    "\n",
    "        # Mark the last point of the stroke\n",
    "        if deltas: # Ensure deltas is not empty\n",
    "             deltas[-1][2] = 1\n",
    "\n",
    "        last_x, last_y = x_coords[-1], y_coords[-1]\n",
    "\n",
    "    # Truncate if longer than MAX_LEN\n",
    "    if len(deltas) > MAX_LEN:\n",
    "        deltas = deltas[:MAX_LEN]\n",
    "\n",
    "    return np.array(deltas, dtype=np.float32)\n",
    "\n",
    "def load_and_preprocess(classes, data_dir, samples_per_class, max_len):\n",
    "    \"\"\"Loads, preprocesses, and pads the data.\"\"\"\n",
    "    all_sequences = []\n",
    "    all_labels = []\n",
    "    label_map = {name: i for i, name in enumerate(classes)}\n",
    "\n",
    "    print(\"Loading and preprocessing data...\")\n",
    "    for i, class_name in enumerate(classes):\n",
    "        file_path = os.path.join(data_dir, f\"{class_name}.ndjson\")\n",
    "        count = 0\n",
    "        try:\n",
    "            with open(file_path, 'r') as f:\n",
    "                for line in f:\n",
    "                    if count >= samples_per_class:\n",
    "                        break\n",
    "                    try:\n",
    "                        drawing = json.loads(line)\n",
    "                        if not drawing['recognized']: # Optional: Skip drawings not recognized by the game\n",
    "                             continue\n",
    "                        if not drawing.get('drawing'): # Check if drawing data exists\n",
    "                             continue\n",
    "\n",
    "                        delta_sequence = strokes_to_deltas(drawing['drawing'])\n",
    "                        if delta_sequence.shape[0] > 1: # Ensure sequence is not empty or just one point\n",
    "                             all_sequences.append(delta_sequence)\n",
    "                             all_labels.append(label_map[class_name])\n",
    "                             count += 1\n",
    "                    except (json.JSONDecodeError, KeyError, IndexError, TypeError) as e:\n",
    "                        # print(f\"Skipping malformed line/drawing in {class_name}: {e}\")\n",
    "                        continue # Skip malformed lines or drawings\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Warning: File not found for class '{class_name}'. Skipping.\")\n",
    "            continue\n",
    "        print(f\"Loaded {count} samples for '{class_name}'\")\n",
    "\n",
    "    if not all_sequences:\n",
    "         raise ValueError(\"No valid sequences loaded. Check data files and paths.\")\n",
    "\n",
    "    print(f\"Total sequences loaded: {len(all_sequences)}\")\n",
    "\n",
    "    # Pad sequences\n",
    "    # padding='pre' might be slightly better for RNNs, but 'post' is common too.\n",
    "    padded_sequences = pad_sequences(all_sequences, maxlen=max_len,\n",
    "                                     padding='post', dtype='float32')\n",
    "\n",
    "    # Convert labels to categorical\n",
    "    categorical_labels = to_categorical(np.array(all_labels), num_classes=len(classes))\n",
    "\n",
    "    # --- Normalization (Standardization) ---\n",
    "    # Calculate mean and std dev only on the delta values (dx, dy)\n",
    "\n",
    "    non_padding_mask = (padded_sequences[:, :, :2] != 0).any(axis=2) # Mask for non-zero dx/dy\n",
    "    dx_dy_values = padded_sequences[:, :, :2][non_padding_mask]\n",
    "\n",
    "    if dx_dy_values.size == 0:\n",
    "         raise ValueError(\"No valid dx/dy values found for normalization. Check data.\")\n",
    "\n",
    "    mean = np.mean(dx_dy_values, axis=0)\n",
    "    std = np.std(dx_dy_values, axis=0)\n",
    "    # Add a small epsilon to std dev to prevent division by zero\n",
    "    std = np.where(std == 0, 1e-6, std)\n",
    "\n",
    "    print(f\"Normalization - Mean (dx, dy): {mean}, Std Dev (dx, dy): {std}\")\n",
    "\n",
    "    # Apply standardization ONLY to dx and dy (first two elements)\n",
    "    # Avoid normalizing the binary pen_state flag\n",
    "    padded_sequences[:, :, 0] = (padded_sequences[:, :, 0] - mean[0]) / std[0]\n",
    "    padded_sequences[:, :, 1] = (padded_sequences[:, :, 1] - mean[1]) / std[1]\n",
    "\n",
    "    # Set padding values back to 0 after normalization\n",
    "    padding_mask_3d = np.repeat(non_padding_mask[:, :, np.newaxis], 3, axis=2)\n",
    "    padded_sequences = np.where(padding_mask_3d, padded_sequences, 0.0)\n",
    "\n",
    "    print(f\"Data shapes - Sequences: {padded_sequences.shape}, Labels: {categorical_labels.shape}\")\n",
    "\n",
    "    return padded_sequences, categorical_labels\n",
    "\n",
    "# --- Load the data ---\n",
    "# Wrap in a try-except block in case loading fails\n",
    "try:\n",
    "    X, y = load_and_preprocess(CLASSES, DATA_DIR, SAMPLES_PER_CLASS, MAX_LEN)\n",
    "except ValueError as e:\n",
    "    print(f\"Error loading data: {e}\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "# --- Create tf.data Datasets (Optional but recommended for large data) ---\n",
    "# If the dataset fits in memory, you can skip this and use numpy arrays directly\n",
    "# For larger datasets, tf.data is much more efficient\n",
    "\n",
    "# Split data (Example: 80% train, 20% validation)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"Train set size: {X_train.shape[0]}, Validation set size: {X_val.shape[0]}\")\n",
    "\n",
    "# Convert to tf.data datasets\n",
    "# train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "# train_dataset = train_dataset.shuffle(buffer_size=X_train.shape[0]).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "# val_dataset = val_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# print(\"tf.data Datasets created.\")\n",
    "# If using numpy arrays directly (for smaller datasets):\n",
    "train_dataset = (X_train, y_train)\n",
    "val_dataset = (X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-26T09:44:40.966164Z",
     "iopub.status.busy": "2025-04-26T09:44:40.965675Z",
     "iopub.status.idle": "2025-04-26T09:44:41.162624Z",
     "shell.execute_reply": "2025-04-26T09:44:41.162051Z",
     "shell.execute_reply.started": "2025-04-26T09:44:40.966145Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"Stroke_LSTM_Classifier\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"Stroke_LSTM_Classifier\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ conv1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">196</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ bn_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">196</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ relu_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">196</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>)             │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">196</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>)             │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv1d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">196</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │          <span style=\"color: #00af00; text-decoration-color: #00af00\">15,424</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ bn_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">196</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ relu_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">196</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">196</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv1d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">196</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)             │          <span style=\"color: #00af00; text-decoration-color: #00af00\">18,528</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ bn_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">196</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ relu_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">196</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)             │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">196</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)             │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">196</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">115,200</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ bn_dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │             <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ relu_dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ output_softmax (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">3,870</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ conv1d_1 (\u001b[38;5;33mConv1D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m196\u001b[0m, \u001b[38;5;34m48\u001b[0m)             │             \u001b[38;5;34m768\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ bn_1 (\u001b[38;5;33mBatchNormalization\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m196\u001b[0m, \u001b[38;5;34m48\u001b[0m)             │             \u001b[38;5;34m192\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ relu_1 (\u001b[38;5;33mReLU\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m196\u001b[0m, \u001b[38;5;34m48\u001b[0m)             │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m196\u001b[0m, \u001b[38;5;34m48\u001b[0m)             │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv1d_2 (\u001b[38;5;33mConv1D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m196\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │          \u001b[38;5;34m15,424\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ bn_2 (\u001b[38;5;33mBatchNormalization\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m196\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m256\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ relu_2 (\u001b[38;5;33mReLU\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m196\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m196\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv1d_3 (\u001b[38;5;33mConv1D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m196\u001b[0m, \u001b[38;5;34m96\u001b[0m)             │          \u001b[38;5;34m18,528\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ bn_3 (\u001b[38;5;33mBatchNormalization\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m196\u001b[0m, \u001b[38;5;34m96\u001b[0m)             │             \u001b[38;5;34m384\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ relu_3 (\u001b[38;5;33mReLU\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m196\u001b[0m, \u001b[38;5;34m96\u001b[0m)             │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m196\u001b[0m, \u001b[38;5;34m96\u001b[0m)             │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m196\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │         \u001b[38;5;34m115,200\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │         \u001b[38;5;34m131,584\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │          \u001b[38;5;34m16,512\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ bn_dense_1 (\u001b[38;5;33mBatchNormalization\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │             \u001b[38;5;34m512\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ relu_dense_1 (\u001b[38;5;33mReLU\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_dense_1 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ output_softmax (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m)                  │           \u001b[38;5;34m3,870\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">303,230</span> (1.16 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m303,230\u001b[0m (1.16 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">302,558</span> (1.15 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m302,558\u001b[0m (1.15 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">672</span> (2.62 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m672\u001b[0m (2.62 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Conv1D, BatchNormalization, ReLU, Dropout, LSTM, Dense, TimeDistributed\n",
    "\n",
    "def build_stroke_lstm_model(input_shape, num_classes):\n",
    "    \"\"\"Builds the Conv1D -> Stacked LSTM -> Dense model.\"\"\"\n",
    "    model = Sequential(name=\"Stroke_LSTM_Classifier\")\n",
    "\n",
    "    # Input Layer\n",
    "    model.add(Input(shape=input_shape, name=\"input_strokes\"))\n",
    "\n",
    "    # --- 1D Convolutional Block ---\n",
    "    # Conv layers act as feature extractors along the sequence dimension\n",
    "    model.add(Conv1D(filters=48, kernel_size=5, strides=1, padding=\"same\", name=\"conv1d_1\"))\n",
    "    model.add(BatchNormalization(name=\"bn_1\"))\n",
    "    model.add(ReLU(name=\"relu_1\"))\n",
    "    model.add(Dropout(0.2, name=\"dropout_1\")) # Regularization\n",
    "\n",
    "    model.add(Conv1D(filters=64, kernel_size=5, strides=1, padding=\"same\", name=\"conv1d_2\"))\n",
    "    model.add(BatchNormalization(name=\"bn_2\"))\n",
    "    model.add(ReLU(name=\"relu_2\"))\n",
    "    model.add(Dropout(0.2, name=\"dropout_2\"))\n",
    "\n",
    "    model.add(Conv1D(filters=96, kernel_size=3, strides=1, padding=\"same\", name=\"conv1d_3\")) # Third conv often helps\n",
    "    model.add(BatchNormalization(name=\"bn_3\"))\n",
    "    model.add(ReLU(name=\"relu_3\"))\n",
    "    model.add(Dropout(0.2, name=\"dropout_3\"))\n",
    "\n",
    "    # --- Stacked LSTM Block ---\n",
    "    # return_sequences=True passes the output of each time step to the next LSTM\n",
    "    # The last LSTM layer usually has return_sequences=False unless followed by TimeDistributed Dense\n",
    "    model.add(LSTM(units=128, return_sequences=True, dropout=0.3, recurrent_dropout=0.3, name=\"lstm_1\"))\n",
    "    model.add(LSTM(units=128, return_sequences=False, dropout=0.3, recurrent_dropout=0.3, name=\"lstm_2\")) # Only final output needed\n",
    "\n",
    "    # --- Dense Classifier Block ---\n",
    "    model.add(Dense(units=128, name=\"dense_1\"))\n",
    "    model.add(BatchNormalization(name=\"bn_dense_1\"))\n",
    "    model.add(ReLU(name=\"relu_dense_1\"))\n",
    "    model.add(Dropout(0.4, name=\"dropout_dense_1\")) # Heavier dropout before final layer\n",
    "\n",
    "    model.add(Dense(units=num_classes, activation='softmax', name=\"output_softmax\"))\n",
    "\n",
    "    return model\n",
    "\n",
    "# --- Build the model ---\n",
    "input_shape = (MAX_LEN, 3) # max_len sequence length, 3 features (dx, dy, pen_state)\n",
    "model = build_stroke_lstm_model(input_shape, NUM_CLASSES)\n",
    "\n",
    "# --- Compile the model ---\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy', tf.keras.metrics.TopKCategoricalAccuracy(k=3)]) # Top-3 accuracy is useful\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T03:08:26.248470Z",
     "iopub.status.busy": "2025-04-27T03:08:26.247995Z",
     "iopub.status.idle": "2025-04-27T03:08:26.255910Z",
     "shell.execute_reply": "2025-04-27T03:08:26.255189Z",
     "shell.execute_reply.started": "2025-04-27T03:08:26.248447Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training...\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "print(\"\\nStarting training...\")\n",
    "checkpoint_filepath = 'path/to/checkpoint/best_stroke_lstm_model.keras'\n",
    "# --- Callbacks ---\n",
    "# Stop training early if validation loss doesn't improve\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, # Increase patience for complex tasks\n",
    "                               restore_best_weights=True, verbose=1)\n",
    "# Reduce learning rate when validation loss plateaus\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5,\n",
    "                              min_lr=1e-6, verbose=1) # Lower min_lr\n",
    "model_checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    mode='min',\n",
    "    verbose=1,\n",
    "    save_weights_only=False) # Set to True to save only weights\n",
    "\n",
    "# --- Fit the model ---\n",
    "# If using tf.data datasets:\n",
    "# history = model.fit(\n",
    "#     train_dataset,\n",
    "#     epochs=EPOCHS,\n",
    "#     validation_data=val_dataset,\n",
    "#     callbacks=[early_stopping, reduce_lr]\n",
    "# )\n",
    "\n",
    "# If using numpy arrays directly:\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=[early_stopping, reduce_lr, model_checkpoint_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-04-27T05:36:53.749Z",
     "iopub.execute_input": "2025-04-27T03:13:35.743380Z",
     "iopub.status.busy": "2025-04-27T03:13:35.742616Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 347ms/step - accuracy: 0.8676 - loss: 0.4464 - top_k_categorical_accuracy: 0.9635\n",
      "Epoch 1: val_loss improved from inf to 0.23792, saving model to /kaggle/working/best_stroke_lstm_model.keras\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m718s\u001b[0m 379ms/step - accuracy: 0.8676 - loss: 0.4464 - top_k_categorical_accuracy: 0.9635 - val_accuracy: 0.9264 - val_loss: 0.2379 - val_top_k_categorical_accuracy: 0.9851 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 348ms/step - accuracy: 0.8709 - loss: 0.4366 - top_k_categorical_accuracy: 0.9642\n",
      "Epoch 2: val_loss improved from 0.23792 to 0.22297, saving model to /kaggle/working/best_stroke_lstm_model.keras\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m711s\u001b[0m 379ms/step - accuracy: 0.8709 - loss: 0.4366 - top_k_categorical_accuracy: 0.9642 - val_accuracy: 0.9317 - val_loss: 0.2230 - val_top_k_categorical_accuracy: 0.9866 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 349ms/step - accuracy: 0.8744 - loss: 0.4224 - top_k_categorical_accuracy: 0.9648\n",
      "Epoch 3: val_loss improved from 0.22297 to 0.22145, saving model to /kaggle/working/best_stroke_lstm_model.keras\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m712s\u001b[0m 380ms/step - accuracy: 0.8744 - loss: 0.4224 - top_k_categorical_accuracy: 0.9648 - val_accuracy: 0.9322 - val_loss: 0.2214 - val_top_k_categorical_accuracy: 0.9872 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 349ms/step - accuracy: 0.8794 - loss: 0.4083 - top_k_categorical_accuracy: 0.9670\n",
      "Epoch 4: val_loss improved from 0.22145 to 0.21388, saving model to /kaggle/working/best_stroke_lstm_model.keras\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m713s\u001b[0m 380ms/step - accuracy: 0.8794 - loss: 0.4083 - top_k_categorical_accuracy: 0.9670 - val_accuracy: 0.9340 - val_loss: 0.2139 - val_top_k_categorical_accuracy: 0.9872 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 345ms/step - accuracy: 0.8794 - loss: 0.4025 - top_k_categorical_accuracy: 0.9687\n",
      "Epoch 5: val_loss improved from 0.21388 to 0.21153, saving model to /kaggle/working/best_stroke_lstm_model.keras\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m706s\u001b[0m 377ms/step - accuracy: 0.8794 - loss: 0.4025 - top_k_categorical_accuracy: 0.9687 - val_accuracy: 0.9352 - val_loss: 0.2115 - val_top_k_categorical_accuracy: 0.9878 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 342ms/step - accuracy: 0.8826 - loss: 0.3944 - top_k_categorical_accuracy: 0.9689\n",
      "Epoch 6: val_loss improved from 0.21153 to 0.20856, saving model to /kaggle/working/best_stroke_lstm_model.keras\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m699s\u001b[0m 373ms/step - accuracy: 0.8826 - loss: 0.3944 - top_k_categorical_accuracy: 0.9689 - val_accuracy: 0.9366 - val_loss: 0.2086 - val_top_k_categorical_accuracy: 0.9879 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 343ms/step - accuracy: 0.8851 - loss: 0.3875 - top_k_categorical_accuracy: 0.9698\n",
      "Epoch 7: val_loss did not improve from 0.20856\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m702s\u001b[0m 374ms/step - accuracy: 0.8851 - loss: 0.3875 - top_k_categorical_accuracy: 0.9698 - val_accuracy: 0.9357 - val_loss: 0.2088 - val_top_k_categorical_accuracy: 0.9878 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 341ms/step - accuracy: 0.8852 - loss: 0.3842 - top_k_categorical_accuracy: 0.9698\n",
      "Epoch 8: val_loss improved from 0.20856 to 0.20330, saving model to /kaggle/working/best_stroke_lstm_model.keras\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m697s\u001b[0m 372ms/step - accuracy: 0.8852 - loss: 0.3842 - top_k_categorical_accuracy: 0.9698 - val_accuracy: 0.9385 - val_loss: 0.2033 - val_top_k_categorical_accuracy: 0.9880 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 340ms/step - accuracy: 0.8904 - loss: 0.3697 - top_k_categorical_accuracy: 0.9720\n",
      "Epoch 9: val_loss did not improve from 0.20330\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m697s\u001b[0m 372ms/step - accuracy: 0.8904 - loss: 0.3697 - top_k_categorical_accuracy: 0.9720 - val_accuracy: 0.9373 - val_loss: 0.2045 - val_top_k_categorical_accuracy: 0.9881 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 341ms/step - accuracy: 0.8898 - loss: 0.3694 - top_k_categorical_accuracy: 0.9717\n",
      "Epoch 10: val_loss improved from 0.20330 to 0.19656, saving model to /kaggle/working/best_stroke_lstm_model.keras\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m697s\u001b[0m 372ms/step - accuracy: 0.8898 - loss: 0.3694 - top_k_categorical_accuracy: 0.9717 - val_accuracy: 0.9398 - val_loss: 0.1966 - val_top_k_categorical_accuracy: 0.9890 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 343ms/step - accuracy: 0.8925 - loss: 0.3589 - top_k_categorical_accuracy: 0.9727\n",
      "Epoch 11: val_loss improved from 0.19656 to 0.19028, saving model to /kaggle/working/best_stroke_lstm_model.keras\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m701s\u001b[0m 374ms/step - accuracy: 0.8925 - loss: 0.3589 - top_k_categorical_accuracy: 0.9727 - val_accuracy: 0.9420 - val_loss: 0.1903 - val_top_k_categorical_accuracy: 0.9891 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 345ms/step - accuracy: 0.8943 - loss: 0.3548 - top_k_categorical_accuracy: 0.9733\n",
      "Epoch 12: val_loss improved from 0.19028 to 0.18859, saving model to /kaggle/working/best_stroke_lstm_model.keras\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m705s\u001b[0m 376ms/step - accuracy: 0.8943 - loss: 0.3548 - top_k_categorical_accuracy: 0.9733 - val_accuracy: 0.9420 - val_loss: 0.1886 - val_top_k_categorical_accuracy: 0.9894 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "\u001b[1m 395/1875\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8:34\u001b[0m 348ms/step - accuracy: 0.8955 - loss: 0.3466 - top_k_categorical_accuracy: 0.9735"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=[early_stopping, reduce_lr, model_checkpoint_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load the saved model\n",
    "model = load_model('path/to/checkpoint/best_stroke_lstm_model.keras')\n",
    "\n",
    "# --- Evaluate the model (optional) ---\n",
    "print(\"\\nEvaluating on validation set:\")\n",
    "loss, accuracy, top3_accuracy = model.evaluate(X_val, y_val, verbose=0)\n",
    "print(f\"Validation Loss: {loss:.4f}\")\n",
    "print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Validation Top-3 Accuracy: {top3_accuracy:.4f}\")\n",
    "\n",
    "# --- Save the model (optional) ---\n",
    "model.save(\"quickdraw_stroke_lstm_model.keras\") # update the path\n",
    "print(\"\\nModel saved as quickdraw_stroke_lstm_model.keras\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7256270,
     "sourceId": 11573735,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 319211,
     "modelInstanceId": 298609,
     "sourceId": 358483,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 319211,
     "modelInstanceId": 298609,
     "sourceId": 358895,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 319211,
     "modelInstanceId": 298609,
     "sourceId": 359615,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Tensorflow (2.19.0)",
   "language": "python",
   "name": "test-qnl_lgol-py3.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
